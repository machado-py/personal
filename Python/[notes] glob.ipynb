{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://pynative.com/python-glob/\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *\t    Matches everything\t                    *.pdf matches all files with the pdf extension\n",
    "# ?\t    Matches any single character\t        sales/??.jpeg matches all files with two characters long present in the sales folder\n",
    "# []\tMatches any character in the sequence.\t[psr]* matches files starting with the letter p, s, or r.\n",
    "# [!]\tMatches any character not in sequence\t[!psr]* matches files not starting with the letter p, s, or r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[class] Seaborn01.ipynb',\n",
       " '[class] Seaborn02.ipynb',\n",
       " '[notes] datetime.ipynb',\n",
       " '[notes] Everything All at Once.ipynb',\n",
       " '[notes] glob.ipynb',\n",
       " '[notes] pivot_highlight, seaborn, sankey chart.ipynb',\n",
       " '[notes] pyautogui.ipynb',\n",
       " '[proj][classification] House Credit.ipynb',\n",
       " '[proj][neural network] Colab TensorFLow Keras.ipynb',\n",
       " '[proj][nlp] Bayes_Shap.ipynb',\n",
       " '[proj][nlp] Grid.ipynb',\n",
       " '[proj][pandas] Data Manipulation.ipynb',\n",
       " '[proj][regression] Edureka.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks for .ipynb files in the current dir only\n",
    "files = glob.glob('*.ipynb')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[class] Seaborn01.ipynb',\n",
       " '[class] Seaborn02.ipynb',\n",
       " '[proj][classification] House Credit.ipynb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('*class*.ipynb')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[class] Seaborn01.ipynb', '[class] Seaborn02.ipynb']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numeric range [0-9]\n",
    "files = glob.glob('*Seaborn[0-9]*.ipynb')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[class] Seaborn01.ipynb',\n",
       " '[class] Seaborn02.ipynb',\n",
       " '[notes] pivot_highlight, seaborn, sankey chart.ipynb']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('*Seaborn??*.ipynb')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\databricks',\n",
       " '..\\\\Python',\n",
       " '..\\\\Simul MonteCarlo',\n",
       " '..\\\\SQL',\n",
       " '..\\\\Web Scraping - scrapy',\n",
       " '..\\\\databricks\\\\anotacoes_aula.ipynb',\n",
       " '..\\\\Python\\\\EDA_v1.py',\n",
       " '..\\\\Python\\\\Tabela Resumo_v2.xlsx',\n",
       " '..\\\\Python\\\\[class] Seaborn01.ipynb',\n",
       " '..\\\\Python\\\\[class] Seaborn02.ipynb',\n",
       " '..\\\\Python\\\\[notes] datetime.ipynb',\n",
       " '..\\\\Python\\\\[notes] Everything All at Once.ipynb',\n",
       " '..\\\\Python\\\\[notes] glob.ipynb',\n",
       " '..\\\\Python\\\\[notes] pivot_highlight, seaborn, sankey chart.ipynb',\n",
       " '..\\\\Python\\\\[notes] pyautogui.ipynb',\n",
       " '..\\\\Python\\\\[proj][classification] House Credit.ipynb',\n",
       " '..\\\\Python\\\\[proj][neural network] Colab TensorFLow Keras.ipynb',\n",
       " '..\\\\Python\\\\[proj][nlp] Bayes_Shap.ipynb',\n",
       " '..\\\\Python\\\\[proj][nlp] Grid.ipynb',\n",
       " '..\\\\Python\\\\[proj][pandas] Data Manipulation.ipynb',\n",
       " '..\\\\Python\\\\[proj][python] MasterMind.py',\n",
       " '..\\\\Python\\\\[proj][regression] Edureka.ipynb',\n",
       " '..\\\\Simul MonteCarlo\\\\InputFile.xlsx',\n",
       " '..\\\\Simul MonteCarlo\\\\main.py',\n",
       " '..\\\\Simul MonteCarlo\\\\output_files',\n",
       " '..\\\\Simul MonteCarlo\\\\requirements.txt',\n",
       " '..\\\\Simul MonteCarlo\\\\setup.py',\n",
       " '..\\\\Simul MonteCarlo\\\\teste.ipynb',\n",
       " '..\\\\Simul MonteCarlo\\\\_Monte_Carlo_v10.py',\n",
       " '..\\\\Simul MonteCarlo\\\\__pycache__',\n",
       " '..\\\\Simul MonteCarlo\\\\output_files\\\\_Model_OutputFile_1.xlsx',\n",
       " '..\\\\Simul MonteCarlo\\\\__pycache__\\\\_Monte_Carlo_v10.cpython-39.pyc',\n",
       " '..\\\\SQL\\\\[class] exercises01.sql',\n",
       " '..\\\\SQL\\\\[class] exercises02.sql',\n",
       " '..\\\\SQL\\\\[class] psycopg2.ipynb',\n",
       " '..\\\\SQL\\\\[class] sql01.sql',\n",
       " '..\\\\SQL\\\\[class] sql02.sql',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias',\n",
       " '..\\\\Web Scraping - scrapy\\\\readme.txt',\n",
       " '..\\\\Web Scraping - scrapy\\\\sites_noticias.txt',\n",
       " '..\\\\Web Scraping - scrapy\\\\teste.ipynb',\n",
       " '..\\\\Web Scraping - scrapy\\\\teste.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\analisar_noticias.ipynb',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias.txt',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\scrapy.cfg',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\aeroflap.json',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\aeroin.json',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\aeromagazine.json',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\airway.json',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\aviacaobrasil.json',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\items.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\middlewares.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\pipelines.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\runallspiders.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\settings.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__init__.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__pycache__',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\aeroflap.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\aeroin.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\aeromagazine.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\airway.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\aviacaobrasil.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__init__.py',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroflap.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroflap_v1.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroin.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroin_old.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroin_v1.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeroin_v2.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aeromagazine.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\airway.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\aviacaobrasil.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\panrotas.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\spiders\\\\__pycache__\\\\__init__.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__pycache__\\\\items.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__pycache__\\\\pipelines.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__pycache__\\\\settings.cpython-39.pyc',\n",
       " '..\\\\Web Scraping - scrapy\\\\noticias\\\\noticias\\\\__pycache__\\\\__init__.cpython-39.pyc']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks for .ipynb files in the current dir and subdirectories (**/) starting from previous directory (../)\n",
    "files = glob.glob('../**/*', recursive=True)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EDA_v1.py', '[proj][python] MasterMind.py', 'Tabela Resumo_v2.xlsx']\n"
     ]
    }
   ],
   "source": [
    "extensions = ('*.py', '*.xlsx')\n",
    "files_list = []\n",
    "for ext in extensions:\n",
    "    files_list.extend(glob.glob(ext))\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "### Look all txt files of current directory and its sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '**/*.txt'\n",
    "search_word = 'profit'\n",
    "\n",
    "# list to store files that contain matching word\n",
    "final_files = []\n",
    "\n",
    "for file in glob.glob(path, recursive=True):\n",
    "    try:\n",
    "        with open(file) as fp:\n",
    "            # read the file as a string\n",
    "            data = fp.read()\n",
    "            if search_word in data:\n",
    "                final_files.append(file)\n",
    "    except:\n",
    "        print('Exception while reading file')\n",
    "\n",
    "print(final_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0d3f0ed3089cd13411b5a2ebe388e4aa58e262e30ca5148dcd7bf493791b26c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
